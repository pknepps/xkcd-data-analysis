---
title: "XKCD: a Semi-Serious Analysis of a Non-Serious Comic"
author: "Preston Knepper and Kevin McCall"
date: "2024-04-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
```

# Introdution

In this study, we will analyze commonalities and frequencies of various items in the xkcd. XKCD is a web comic written by Randall Munroe, a CMU graduate with a degree in physics. Before writing comics full time, he previously had worked on robotics at NASA.

XKCD refers to itself as "A web-comic of romance, sarcasm, math, and language", taking its roots in nerdy humor typically about technology, math, physics and science. XKCD is popular in the world covered by those topics. The chances of you being in this class (Math 474) and never seeing an xkcd comic are slim to none. The comic was originally started on LiveJournal on September 30, 2005, with an upload of 13 images. It quickly gain popularity and it was moved to his own server, xkcd.com, on January 1, 2006. Since then, the comic has been posted regularly on every Monday, Wednesday, and Saturday. 

The web-comic features many topics and loved characters (despite the comic made up of almost entirely stick figures). These topics are mostly STEM related, but can also be about politics, language, and sometimes just feature bizarre and random humor -- like most comics featuring Beret Guy. Some popular characters from the comic include [Cueball](https://www.explainxkcd.com/wiki/index.php/Cueball), [Black Hat](https://www.explainxkcd.com/wiki/index.php/Black_Hat), the previously mentioned [Beret Guy](https://www.explainxkcd.com/wiki/index.php/Beret_Guy), and Cueball's girlfriend, [Megan](https://www.explainxkcd.com/wiki/index.php/Megan). For the most part, the characters have little story and are mostly used to expand on the humor in an individual comic.

In this paper, we will observe the transcripts of these comics, as well as unique properties about some of the comics and the dates they were posted.

# The data
We will be using the tidyverse package throughout this project. We will go ahead and include it now. Another package we use later is the 'ggwordcloud' package
```{r packages, message=FALSE}
if (!require("tidyverse")) {
    install.packages("tidyverse")
}
if (!require("ggwordcloud")) {
    install.packages("ggwordcloud")
}
if (!require("MASS")) {
    install.packages("MASS")
}
library("tidyverse")
library(MASS)
library(ggwordcloud)

```

The .csv below contains data on every single xkcd comic. Most importantly, the transcript of every comic. It's a good idea to run the python file, extractDataset.py, before viewing this file for the first time, as a comic is posted three times every week. Monroe offers an API for his comic, which only requires the parsing of jsons into a .csv which R can read.
```{r csv}
xkcd <- read.csv("xkcd.csv", quote = "")
nrow(xkcd)
```
From the summary, we get information on the metadata of the comic. A comic has the following attributes:

* num: The comic number, this is simply a unique key assigned chronologically.
* year, month, day: the date the comic was posted. All as integers.
* title: The title of the comic.
* safe_title: The title of the comic. A section of this paper will be dedicated to finding differences between the two.
* transcript: A transcript of this comic. Includes dialog and image descriptions.
* alt: The alt-text found by mousing over the comic.
* img: A url to the image of the comic.
* link: An link to an external site found by clicking the comic. Few comics utilize this.
* news: News updates. Rare, but typically about upcoming publications.
* extra_parts: Occasionally, Monroe will post a game, or other type of interactive comic. Information about it will be posted as a json here. For simplicities sake, this attribute contains the first value of said nested json.

# Cleaning the data

The first thing to do is to convert the three separate attributes year, month, day into a single value date_posted. It will also be useful to add a day_of_week attribute, which includes the day of the week the comic was posted. The day of the week will probably be more useful than just the date as the comic is posted regularly on Mondays, Wednesdays, and Fridays.

```{r date_parse}
xkcd_date <- xkcd |> unite(date, month, day, year, sep = "-") |> mutate(date = as.Date(mdy(date))) |> mutate(weekday = wday(date, label = TRUE),.after = date)
get_year <- function(x) {
    return(format(x, "%Y"))
}
```
Another thing to note is that the transcript includes a physical description of the comic itself. Sorry blind readers, but we do not care for this. We will remove these parts using regex's. Note Image descriptors are wrapped in "[[]]". Note that the alt text is also included here (wrapped in "\{\{\}\}"), since there is already a section for alt text, we will remove it. The final part to remove are the screenplay names (e.g. "Man:", "Woman:") as they only indicate who is speaking, and are not part of the actual text of the comic.

## Parsing the transcript

![I_Know_Regular_Expressions](https://xkcd.com/s/0b7742.png)

Okay, time to tidy transcript.

There are 2 challenges in doing this 
- scene-building is described in [[]]
- Dialogue is in the format Person: says something (we thought)

We will now remove the alt text, scene-building, and extra new lines. Then we divide the remaining transcript into a matrix where the 2 columns are [character, dialog]. Then we reduce the matrices into two values of [characters, text], and turn the total list of values into a dataframe.
This will be done using regular expressions. The expressions are all strings starting in 'r' below, with their variable names giving their respective parsing.

```{r dialog_parse_fn}
view_xkcd_string <- function(strin) {
    str_replace_all(strin, r"{\\n}", "\n") |> str_view()
}


scenebuilding = r"{\[\[([^\]]*)\]\]}"
dialog_regex = r"{(?:\\n)?([^:]+): ((?:.(?!\\n[^:]+:))*.)}"
# THIS IS THE NAME REGEX
names_regex = r"{(?:\\n)(?=(?:.(?!\\n))+:)([^:]+):}"
bad_newlines = r"{(?:\\n)(?!(?:.(?!\\n))+:)}"

parse_dialog <- function(dialog) {
    step0 = str_replace_all(dialog, r"{(?:\\n)+}", r"{\\n}")
    step1 = str_remove_all(step0, r"(^\s+"|\{\{.*\}\}"?$)")
    scene_info = str_match_all(step1, scenebuilding)
    step2.1 = str_remove_all(step1, scenebuilding)
    step2.2 = str_replace_all(step2.1, r"{(?:\\n)+}", r"{\\n}")
    step2.3 = str_remove(step2.2, r"(\s*(?:\\n)+$)")
    names = str_match_all(step2.3, names_regex)
    dialog = str_split(step2.3, names_regex)
    n <- names |> map(\(x) reduce(x[,2], function(x, y) paste(x, y, sep = "|||"), .init="")) |> unlist()
    d <- dialog |> map(\(x) reduce(x, paste, .init="")) |> unlist()
    s <- scene_info |> map(\(x) reduce(x[,2], paste, .init="")) |> unlist()
    data.frame(names = n, dialog= d, scene=s)
}
```

We now need to just concatenate the new dataframe onto our data.

```{r dialog_parse}
xkcd_comma <- xkcd_date |> mutate(transcript = transcript |> str_replace_all("\\(COMMA\\)", ","))

xkcd_transcript <- xkcd_comma |> cbind(xkcd_comma |> pull(transcript) |> parse_dialog())
head(xkcd_transcript)
```


# Analysis
## Dates

The first thing we can analyze without any data manipulation is the frequency of comics across each year and weekday.
```{r comicsByYear}
# we extract the year from the given date by using format(date), "%y"
xkcd_date |> mutate(year = get_year(date)) |> ggplot(aes(year)) + geom_bar(fill = "brown") + labs(title = "Number of Comics Posted by Year")
```
```{r comicsByWeekday}
# graph for posts by day of the week
xkcd_date |> ggplot(aes(weekday)) + geom_bar(fill = "brown") + labs(title = "Number of Comics Posted by Weekday")
```

As we see, the yearly posts are pretty consistent. While 2024 provides no significance (as the year is not complete), 2006 does. We can investigate.
```{r 2006}
xkcd_date |> filter(get_year(date) == "2006") |> group_by(date) |> count() |> arrange(desc(n)) |> head()
```
We can see here that Munroe posted 44 comics on January 1, 2006. Which is the day xkcd was started. To quote Munroe at this time, "I was going through old math/sketching graph paper notebooks and didn't want to lose some of the work in them, so I started scanning pages. I took the more comic-y ones and put them up on a server I was testing out."

Now to discuss the second graph. While xkcd has had the majority of it's posts on MWF, there are some exceptions. We already know from the previous graph, that 44 comics were posted on 6-1-1, a sunday. Lets look at some of the other dates when this happened.
```{r unusual_dates}
xkcd_date_abnormal <- xkcd_date |> filter(weekday %in% c("Sun", "Tue", "Thu", "Sat"), date != "2006-01-01")
xkcd_date_abnormal |> group_by(weekday) |> count()
```
So 3 of the sunday comics were not posted on the first day. Meaning we have 34 abnormal posts. Lets narrow down the time of theses abnormalities a bit further.
```{r more_unusual_dates}
xkcd_date_abnormal |> group_by(get_year(date), weekday) |> count()
```
It is not a surprise to see that the majority of date abnormalities are in 2006, when the comic was not full-time. For interest's sake, here are a few of the later comics which were abnormal.
```{r list_of_unusual_comics}
(xkcd_date_abnormal |> filter(get_year(date) > 2018))[c(1:4, 8:10)]
```
<p float="left">
    <img src="https://imgs.xkcd.com/comics/throw.png" alt="throw" width="19%"/>
    <img src="https://imgs.xkcd.com/comics/scenario_4.png" alt="scenario_4" width="19%"/>
    <img src="https://imgs.xkcd.com/comics/everyones_an_epidemiologist.png" alt="everyones_an_epidemiologist" width="19%"/>
    <img src="https://imgs.xkcd.com/comics/checkbox.gif"alt="checkbox" width="19%"/>
    <img src="https://imgs.xkcd.com/comics/what_if_2_flowchart.png" alt="What if 2 flowchart" width="19%"/>
</p> 
Interestingly enough, all but one of the last 5 comics which were off-schedule were "special" comics (a term we will define later) and/or promotions for Munroe's most recent publication (in these cases, <ins>What if? 2</ins> and <ins>How to</ins> where promoted).

## Special comics

Speaking of special comics. Lets look at some of those. In this project, we define special comics as any unusual post that isn't just a comic. The primary form of this are posts where the extra_parts fields are not null, that is the comic is an interactive comic. It could be a game, a gif, or a movie. We will also be looking at comics with links and comics where some news was announced. 
```{r}
specialComics <- xkcd_date |> filter(extra_parts != " NA")
hasNews <- xkcd_date |> filter(news != " NA")
hasLink <- xkcd_date |> filter(link != " NA")
special_union <- union(union(specialComics, hasNews), hasLink)
nrow(specialComics)
nrow(hasNews)
nrow(hasLink)
nrow(special_union)
nrow(intersect(specialComics, hasNews))
nrow(intersect(specialComics, hasLink))
nrow(intersect(hasNews, hasLink))
nrow(intersect(specialComics, intersect(hasNews, hasLink)))
```

So there are 23 special comics. 54 comics with news announced, and 72 comics which have a link attached to the picture. This totals to 125 special comics (as some comics fall under more than one of the 3 categories). We see that there are 9 comics which are both special and have news, 3 comics which are special and have a link, 12 comics which have both news and a link, and 0 comics with all 3 qualities.

Lets look at how frequent these comics were over time.

```{r}
special_union |> mutate(year = format(date, "%Y")) |> ggplot(aes(year)) + geom_bar(fill = "orange") + labs(title = "Comics with Unique Properties by Year")
```

Here's how frequent each attribute was over time.

```{r, figures-side, fig.show="hold", out.width="33%"}
specialComics |> mutate(year = get_year(date)) |> ggplot(aes(year)) + geom_bar(fill = "red") + labs(title = "Special Comics by Year")
hasLink |> mutate(year = get_year(date)) |> ggplot(aes(year)) + geom_bar(fill = "green") + labs(title = "Comics with Links by Year")
hasNews |> mutate(year = get_year(date)) |> ggplot(aes(year)) + geom_bar(fill = "blue") + labs(title = "Comics with News by Year")
```
We see here that the majority of the comics with extra features peaked around 2012, and had a spike in 2022. This is true for the individual properties as well, with the exception that news did not have a spike in 2022.

Lets go back a bit to where we analyzed the comics that were posted on Tuesdays and Thursdays. We can try to prove our hypothesis that most of these are our special comics. 
```{r}
special_union |> ggplot(aes(weekday)) + geom_bar(fill = "purple") + labs(title = "Comics with Unique Properties by Weekday")
nrow(union(special_union, xkcd_date_abnormal))
nrow(intersect(special_union, xkcd_date_abnormal)) / nrow(xkcd_date_abnormal)
nrow(special_union) / nrow(xkcd)
```
We see here that the distribution looks fairly similar to the previous weekday graph. However looking at the numbers, we see that there are 145 comics with unique properties that were posted on abnormal days. That's 41.1% of all comics posted on abnormal days. Considering comics with unique properties make up about 4.3% of all comics. This means unique comics make up a significant portion of comics posted on off-days.

## Titles
One thing mentioned in Section 1: The Data is that there didn't seem to be a difference between 'title' and 'safe_title'. Let's look at that now.

```{r}
xkcd_title_diff <- xkcd_date |> filter(title != safe_title)
xkcd_title_diff[1:5]
```
One of these comics has an accent above the 'e' in the word 'Clichéd' which may not be readable by some browsers. The other has a blue fill for the word 'House', which once again, may not be readable by some browsers. It is interesting that the answer for this problem was to create an entire new field for this problem, when it could have been handled like the 'extra_parts' item, where only some of the comics even register one (we put in a NA value for all comics which had no 'extra_parts' attribute).
<p float="left">
    <img src="https://imgs.xkcd.com/comics/cliched_exchanges.png" alt="cliched_exchanges" width="49%"/>
    <img src="https://imgs.xkcd.com/comics/house_of_pancakes.png" alt="house_of_pancakes" width="49%"/>
</p> 

## The transcript
Now for the part we've all been waiting for. After expending much effort into parsing the transcript with regular expressions, we will finally use it. Starting with the most common 100 words used in his comic.

```{r}
xkcd_words <- xkcd_transcript |> separate_longer_delim(dialog, delim = regex(r"(\s+)")) |> mutate(dialog = tolower(dialog) |> str_replace_all("[^[\\w+]]", "") |> na_if("")) |> filter(!is.na(dialog))
word_frequency <- xkcd_words |> group_by(dialog) |> count() |> arrange(desc(n))
```

To no one's suprise, the most common words are the articles 'the', 'a', 'of', etc.. 

Let's now look at the most common character's in the comic.
```{r}
xkcd_characters <- xkcd_transcript |> separate_longer_delim(names, delim = "|||") |> mutate(names = str_squish(names) |> tolower() |> na_if("")) |> filter(!is.na(names))
character_freq <- xkcd_characters |> group_by(names) |> count() |> arrange(desc(n))
head(character_freq)
black_hat_guys <- character_freq |> filter(str_detect(names, ".*hat.*")) |> filter(str_detect(names, ".*black.*")); black_hat_guys
black_hat_guys |> ungroup() |> summarize(black_hat_count = sum(n))
```

Most of these names are generic (man, woman, etc.). However, the notable character's within the top 20 are 'Black Hat Guy' and 'Beret Guy'. As mentioned earlier, these characters have the most consistent personality and are some of the favorite characters of the comic. Otherwise, it seems that the naming of characters is very inconsistent, with seven different entries for 'Black Hat Guy' in the first 150 most common character names: "man in black hat", "guy with hat", "black hat", "black hat man", "hat man", "black hat guy", "hat guy". This makes it hard to perform any actual analysis on this data. However, there are still some interesting things to pull from here. For example we see an entry for 'randall' (the comics' author), let's look into those comics.


Let's see how good of a job we did parsing the comics.

The xkcd comics have a wiki where people compile information about the comics. Let's take a look at hat guy once again.

According to the wiki https://www.explainxkcd.com/wiki/index.php/Black_Hat, Black hat makes 174 appearances. With our regex, we have captured 101. For Black Hat, we successfully recognize 58% of his occurances.

```{r}
black_hat <- xkcd_characters |>
    filter((str_detect(names, "\\bhat\\b") & str_detect(names, "black|man|guy")) | (str_detect(scene, "\\bhat\\b") & str_detect(scene, "black|man|guy"))) |> 
    filter(!str_detect(names, "boy|white")) |> group_by(num, date, weekday, title) |> count()
black_hat |> mutate(year = get_year(date)) |> ggplot(aes(year)) + geom_bar(fill = "black") + 
    labs(title = "Black Hat Man Appearances Over Time")
```

We see a general downward trend. This makes sense as Black Hat was an early character who had most repetitive appearances during the early years of the comic. However, we notice that his appearances just cut off after 2016. However, we see Black Hat recently in comic 2900: Call My Cell on February 28, 2024. A look at this comic shows that there is no transcript. Interesting.

```{r}
beret <- xkcd_characters |> filter(str_detect(names, "beret") | str_detect(scene, "beret")) |> group_by(num, date, weekday, title) |> count()
beret |> mutate(year = get_year(date)) |> ggplot(aes(year, fill = year)) + geom_bar(show.legend = FALSE) + 
    labs(title = "Beret Man Appearances Over Time")
```
Now we are going to fix these inconsistencies.
```{r}
fix_beret <- function(x) {
  if_else(str_detect(x, "beret"),
          "Beret Guy",
          x)
}

fix_black_hat <- function(x) {
  if_else(str_detect(x, "\\bhat\\b") & str_detect(x, "black|man|guy"),
          "Black Hat",
          x)
}

test <- c("beret", "amongus", "black hat")

fix_beret(test)
fix_black_hat(test)

xkcd_characters <- xkcd_characters |> mutate(names = fix_beret(names), names = fix_black_hat(names))
```



With Beret man, we see a reverse trend over Black Hat. Beret man is one of Munroe's favorite characters, so the slow increase in appearances makes sense. Once again, though, we see a sudden cutoff of appearances after 2015. Investigating again we see missing transcripts.

## Comics with no transcripts
```{r}
no_transcript <- xkcd_transcript |> filter(transcript == " NA")
no_transcript |> mutate(year = get_year(date)) |> ggplot(aes(year, fill = year)) + geom_bar(show.legend = FALSE ) + labs(title = "Comics Without Transcripts Over Time")
no_transcript |> filter(get_year(date) == 2015)
nrow(no_transcript) / nrow(xkcd)
```

Turns out, Munroe stopped writing transcripts in 2016, not a single transcript was missed until 2015. Then the transcripts stopped overall. This was a very unfortunate surprise to our already unfortunate experience parsing the transcript. This is about 43% of the database missing transcripts. However, we still have data from the first 10 years of the comic.

### Linear Descriminant Analysis (LDA)

Linear Descriminant analysis allows us to create a model which will determine likelyhood of a catagorical variable. In this case, we are trying to predict if Black Hat is in a comic or not.

```{r}
xkcd_for_fit <- (xkcd_transcript |> mutate(has_black_hat = num %in% black_hat$num, has_beret = num %in% beret$num))[1:1500,]
sample <- sample(1500, 750)
xkcd_sample <- xkcd_for_fit[sample,]
xkcd_test <- xkcd_for_fit[-sample,]
lda.fit <- lda(has_black_hat ~ date, data = xkcd_sample)
lda.fit
lda.pred <- predict(lda.fit, data = xkcd_test)
names(lda.pred)
lda.class = lda.pred$class
table(lda.class, xkcd_test$has_black_hat)
```

This didn't end up working as the model always chose false.

### Word Clouds

Words clouds are a good way to visualize trends in categorical variables. Here are word clouds summarizing
the data from XKCD.

```{r wordclouds}
parsed_comic <- xkcd_comma |> pull(transcript) |> parse_dialog() |> tibble()

remove_characters_regex <- r"{\s+|<[^>]*>+|\\n|[\\\/\~\@\#\$\%\^\&\*\(\)\-\_\=\+\[\]\}\{\,\.\?\/\"\;\:\|\>\<\!]}"

set.seed(42)

# Top 10 most frequent characters in the dataset
top_10_characters <- xkcd_characters |>
  group_by(names) |>
  count(names) |>
  arrange(desc(n)) |>
  head(10)

words_said <- xkcd_characters |>
  dplyr::select(c(num, names, date, dialog)) |>
  mutate(dialog=tolower(dialog)) |>
  separate_longer_delim(dialog, regex(remove_characters_regex)) |>
  filter(dialog != "" & dialog != "na") |>
  rename(words=dialog)

word_distribution <- words_said |>
  group_by(words) |>
  summarise(word_count=n()) |> 
  mutate(dist = cume_dist(word_count)); word_distribution

character_words_said <- words_said |> 
  semi_join(top_10_characters, join_by(names)) |>
  group_by(names, words) |>
  summarise(word_count=n()) |>
  mutate(rank = dense_rank(desc(word_count)),
         dist = cume_dist(word_count)); character_words_said

character_unusual_words <- character_words_said |>
  inner_join(word_distribution, join_by(words)) |>
  mutate(percent_difference = (dist.x-dist.y) / dist.y) |>
  filter(percent_difference > .5) |>
  mutate(rank = dense_rank(desc(percent_difference))) |>
  arrange(rank)
character_unusual_words

all_words <- parsed_comic$scene |>
  tolower() |>
  append(parsed_comic$dialog |> tolower()) |>
  lapply(\(x) str_split(x, remove_characters_regex, simplify = TRUE)) |>
  unlist()
all_words <- all_words[all_words != "" & all_words != "na"]

all_words_tibble <- tibble(words=all_words) |>
  group_by(words) |>
  summarise(word_count=n()) |> 
  arrange(desc(word_count))


# The most common words
all_words_tibble |>
  filter(word_count > 100) |>
  ggplot(aes(label = words, size = word_count)) +
  geom_text_wordcloud() +
  scale_size_area(max_size = 30) +
  theme_minimal()

# Random sample of words
all_words_tibble |>
    sample_n(100, weight = word_count) |>
    ggplot(aes(label=words, size=word_count)) +
    geom_text_wordcloud() +
    scale_size_area(max_size = 30) +
    theme_minimal()


character_words_said |>
  filter(rank <= 20) |>
  ggplot(aes(label = words, size = word_count)) +
  geom_text_wordcloud() +
  facet_wrap(~names) +
  scale_size_area(max_size = 12) +
  theme_minimal() +
  labs(title="Top 20 words said by each character")

top_10_characters

for (character in top_10_characters |> pull(names)) {
  plot <- character_unusual_words |>
    ungroup() |>
    filter(names==character) |>
    slice_head(n=20) |>
    ggplot(aes(label = words, size = percent_difference)) +
    geom_text_wordcloud() +
    # scale_size_area(max_size = 5) +
    theme_minimal() +
    labs(title=paste("Unusual words for ", character), subtitle = "Words that are 50% more unlikely compared to the overall words")
  print(plot)
}



```

# Conclusion

Overall, despite spending a significant portion of time parsing the transcripts, we found very little accurate information. This is mostly due to the inconsistencies from the transcript. These include the fact that parsing the character names would also pull any text which was followed with a ':'. Unfortunately, there isn't a way around this besides separating the transcripts of all 2900+ comics by hand. Another problem, as mentioned previously, is that the naming of characters is extremely inconsistent, making grouping by characters extremely ineffective. some comics didn't even have transcript when they should have, leaving the entry blank. Overall, the transcripts were inconsistent and not ready for data analysis, and even after considerable amounts of cleaning, still yeilded inconsistencies.

# Sources

- xkcd.com
- xkcd.com/json.html
- xkcd.com/about/
- explainxkcd.com